# assistant.py

import os
from dotenv import load_dotenv
from together import Together
import json

# Import device simulators
from devices import Lamp, ACUnit, Television

# Import function definitions
from functions_definitions import TOOLS

# Import data connectors
from data_connectors import get_current_weather, get_latest_news, get_current_date_time

# --- Configuration and Initialization ---
# Load environment variables (like TOGETHER_API_KEY)
load_dotenv()
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")

if not TOGETHER_API_KEY:
    raise ValueError("TOGETHER_API_KEY not found in .env file. Please set it.")

# Initialize TogetherAI client globally
client = Together(api_key=TOGETHER_API_KEY)

# --- Initialize Home Devices ---
# Create instances of our simulated devices globally
home_devices = {
    "lamps": {
        "kitchen": Lamp("Kitchen"),
        "bathroom": Lamp("Bathroom"),
        "room 1": Lamp("Room 1"),
        "room 2": Lamp("Room 2"),
    },
    "ac_units": {
        "room 1": ACUnit("Room 1"),
        "kitchen": ACUnit("Kitchen"),
    },
    "televisions": {
        "living room": Television("Living Room"),
    }
}


# --- Function Execution Logic ---
def execute_tool_call(tool_call: dict) -> str:
    """
    Executes a specific tool call generated by the LLM.
    This function acts as the bridge between the LLM's suggestion and actual device/data control.
    """
    function_name = tool_call["function"]["name"]
    function_args = tool_call["function"]["arguments"]

    print(f"\n--- Executing Tool Call: {function_name} with arguments: {json.dumps(function_args)} ---")

    # --- Device Functions ---
    device_obj = None
    location = function_args.get("location")  # Location is common for device functions

    if function_name.startswith("turn_on_lamp") or function_name.startswith("turn_off_lamp") or \
            function_name.startswith("get_lamp_status"):
        if location and location in home_devices["lamps"]:
            device_obj = home_devices["lamps"][location]
            method_name = function_name.replace("_lamp", "")  # e.g., "turn_on_lamp" -> "turn_on"
            method = getattr(device_obj, method_name)
            if 'location' in function_args:
                del function_args['location']
            result = method(**function_args)
            print(f"--- Device Execution Result: {result} ---")
            return result
        else:
            return f"Error: Lamp not found at location '{location}'."

    elif function_name.startswith("turn_on_ac") or function_name.startswith("turn_off_ac") or \
            function_name.startswith("set_ac_temperature") or function_name.startswith("get_ac_status") or \
            function_name.startswith("get_ac_temperature"):
        if location and location in home_devices["ac_units"]:
            device_obj = home_devices["ac_units"][location]
            method_name = function_name.replace("_ac", "")
            method = getattr(device_obj, method_name)
            if 'location' in function_args:
                del function_args['location']
            result = method(**function_args)
            print(f"--- Device Execution Result: {result} ---")
            return result
        else:
            return f"Error: AC unit not found at location '{location}'."

    elif function_name.startswith("turn_on_tv") or function_name.startswith("turn_off_tv") or \
            function_name.startswith("change_tv_channel") or function_name.startswith("mute_tv") or \
            function_name.startswith("unmute_tv") or function_name.startswith("get_tv_status") or \
            function_name.startswith("get_tv_channel"):
        # TV has only one instance, its location is fixed as 'living room'
        device_obj = home_devices["televisions"]["living room"]
        if location and location != "living room":
            return f"Error: TV is only in the living room, cannot control TV at '{location}'."

        method_name = function_name.replace("_tv", "")
        method = getattr(device_obj, method_name)
        if 'location' in function_args:
                del function_args['location']
        result = method(**function_args)
        print(f"--- Device Execution Result: {result} ---")
        return result

    # --- Data Connector Functions ---
    elif function_name == "get_current_weather":
        result = get_current_weather(location=function_args.get("location"))
        print(f"--- Data Connector Result: {result} ---")
        return result
    elif function_name == "get_latest_news":
        query = function_args.get("query", "general")  # Default to "general" if LLM doesn't provide query
        language = function_args.get("language", "en")  # Default to "en" if LLM doesn't provide language
        result = get_latest_news(query=query, language=language)
        print(f"--- Data Connector Result: {result} ---")
        if 'location' in function_args:
                del function_args['location']
        return result
    elif function_name == "get_current_date_time":
        result = get_current_date_time()
        print(f"--- Data Connector Result: {result} ---")
        return result

    # If no matching function is found
    return f"Error: Unknown function or unsupported command: {function_name}"


def chat_with_assistant(user_message: str):
    """
    Main function to interact with the Smart Home Assistant.
    Processes user messages, interacts with LLM, executes tools, and returns a response.
    """
    messages = [
        {"role": "system",
         "content": "You are a helpful smart home assistant. Control home devices and answer questions about them. Use the provided tools when necessary. Always provide a natural language response."},
        {"role": "user", "content": user_message}
    ]

    try:
        response = client.chat.completions.create(
            model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",  # Using 8B model for faster local testing. Change to 70B if preferred.
            messages=messages,
            tools=TOOLS,  # Provide the list of available tools
            tool_choice="auto"  # Let the LLM decide if it needs to use a tool
        )

        # Check if the LLM decided to call a tool
        if response.choices[0].message.tool_calls:
            tool_calls = response.choices[0].message.tool_calls

            # Append the tool call message from the LLM to the conversation history
            messages.append(response.choices[0].message)

            # Execute each tool call proposed by the LLM
            # For simplicity, we process them sequentially. In complex agents, this might be parallel.
            tool_outputs = []
            for tool_call in tool_calls:
                if isinstance(tool_call, str):
                    tool_call_data = json.loads(tool_call)
                elif hasattr(tool_call, "model_dump"):
                    tool_call_data = tool_call.model_dump()
                else:
                    tool_call_data = tool_call  # فرض بر این که دیکشنری است
                function = tool_call_data.get("function", {})
                arguments_str = function.get("arguments", "{}")
                arguments_dict = json.loads(arguments_str)  # اینجا تبدیل رشته به دیکشنری

                # حالا می‌توانی داده‌ها را برای اجرا آماده کنی
                tool_call_data["function"]["arguments"] = arguments_dict

                print(f"--- Executing Tool Call: {function.get('name')} with arguments: {arguments_dict} ---")

                tool_output = execute_tool_call(tool_call_data)
                # tool_output = execute_tool_call(tool_call.model_dump())
                tool_outputs.append({
                    "role": "tool",
                    "content": tool_output,
                    "tool_call_id": tool_call.id
                })

            # Append the tool outputs to the conversation history
            messages.extend(tool_outputs)

            # Send the messages again with the tool output(s) to get the final response from LLM
            final_response = client.chat.completions.create(
                model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",  # Use the same model
                messages=messages
            )
            return final_response.choices[0].message.content
        else:
            # If no tool call, LLM provides a direct text response
            return response.choices[0].message.content

    except Exception as e:
        print(f"An error occurred in chat_with_assistant: {e}")
        return f"An error occurred while processing your request: {e}. Please try again."


# --- Interactive CLI for testing (only runs when assistant.py is executed directly) ---
if __name__ == "__main__":
    print("به دستیار خانه هوشمند خوش آمدید. دستورات خود را تایپ کنید. برای خروج 'exit' را تایپ کنید.")

    while True:
        user_input = input("شما: ")
        if user_input.lower() == 'exit':
            break

        response = chat_with_assistant(user_input)
        print(f"دستیار: {response}")